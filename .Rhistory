dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params, size = 60 #Indicates the size of the search space
)
xgboost_grid
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(stranded_class ~ .)
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples  = strand_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, roc_auc),
control = tune::control_grid(verbose=TRUE)
)
xgboost_model <-
parsnip::boost_tree(
mode = "classification",
trees=1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params, size = 60 #Indicates the size of the search space
)
xgboost_grid
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(stranded_class ~ .)
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples  = strand_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, roc_auc),
control = tune::control_grid(verbose=TRUE)
)
xgboost_tuned %>%
tune::show_best(metric="roc_auc")
xgboost_best_params <- xgboost_tuned %>%
tune::select_best(metric="roc_auc")
xgboost_model_final <- xgboost_model %>%
tune::finalize_model(xgboost_best_params)
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
View(train_prediction)
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred)
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create training set
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create testing set
test_proc <- bake(strand_rec_preped,
new_data = testing(split))
test_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = test_proc) %>%
bind_cols(training(split))
# Create training set
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create testing set
test_proc <- bake(strand_rec_preped,
new_data = testing(split))
test_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = test_proc
) %>%
predict(new_data = test_proc) %>%
bind_cols(training(split))
# Create training set
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create testing set
test_proc <- bake(strand_rec_preped,
new_data = testing(split))
test_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = test_proc)
View(test_prediction)
View(xgboost_score_train)
# Create training set
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create testing set
test_proc <- bake(strand_rec_preped,
new_data = testing(split))
test_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = test_proc)
# Bind test predictions to labels
cbind(test_prediction, testing(split))
# Create training set
train_proc <- bake(strand_rec_preped,
new_data = training(split))
train_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = train_proc) %>%
bind_cols(training(split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
# Create testing set
test_proc <- bake(strand_rec_preped,
new_data = testing(split))
test_prediction <- xgboost_model_final %>%
fit(
formula = stranded_class ~ .,
data = train_proc
) %>%
predict(new_data = test_proc)
# Bind test predictions to labels
test_prediction <- cbind(test_prediction, testing(split))
xgboost_score <-
test_prediction %>%
yardstick::metrics(stranded_class, .pred_class)
View(xgboost_score)
install.packages("ConfusionTableR")
install.packages("ConfusionTableR")
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(readr)
library(broom)
library(broom.mixed)
library(skimr)
library(remotes)
library(dplyr)
library(magrittr)
library(parallel)
library(doParallel)
library(vip)
library(themis)
library(plotly)
library(ConfusionTableR)
ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class)
View(test_prediction)
ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class,
positive="Stranded")
ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class)
cm_outputs <- ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class)
cm_outputs$
library(data.table)
cm_outputs <- ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class)
data.table::fwrite(cm_outputs$record_level_cm,
file=paste0("Data/xgboost_results_", as.character(Sys.time()),
".csv"))
library(data.table)
library(ConfusionTableR)
cm_outputs <- ConfusionTableR::binary_class_cm(train_labels = test_prediction$.pred_class,
truth_labels = test_prediction$stranded_class)
data.table::fwrite(cm_outputs$record_level_cm,
file=paste0("Data/xgboost_results_", as.character(Sys.time()),
".csv"))
strand_fitted <- strand_fit %>%
extract_fit_parsnip() %>%
tidy()
print(strand_fitted)
# Add significance column to tibble using mutate
strand_fitted <- strand_fitted  %>%
mutate(Significance = ifelse(p.value < 0.05, "Significant", "Insignificant")) %>%
arrange(desc(p.value))
#Create a ggplot object to visualise significance
plot <- strand_fitted %>%
ggplot(data = strand_fitted, mapping = aes(x=term, y=p.value, fill=Significance)) +
geom_col() + theme(axis.text.x = element_text(
face="bold", color="#0070BA",
size=8, angle=90)
) + labs(y="P value", x="Terms",
title="P value significance chart",
subtitle="A chart to represent the significant variables in the model",
caption="Produced by Gary Hutson")
#print("Creating plot of P values")
#print(plot)
plotly::ggplotly(plot)
#print(ggplotly(plot))
#ggsave("Figures/p_val_plot.png", plot) #Save the plot
class_pred <- predict(strand_fit, test_data) #Get the class label predictions
prob_pred <- predict(strand_fit, test_data, type="prob") #Get the probability predictions
lr_predictions <- data.frame(class_pred, prob_pred) %>%
setNames(c("LR_Class", "LR_NotStrandedProb", "LR_StrandedProb")) #Combined into tibble and rename
stranded_preds <- test_data %>%
bind_cols(lr_predictions)
print(tail(lr_predictions))
roc_plot <-
stranded_preds %>%
roc_curve(truth = stranded_class, LR_NotStrandedProb) %>%
autoplot
print(roc_plot)
roc_plot <-
stranded_preds %>%
roc_curve(truth = stranded_class, stranded_preds$LR_StrandedProb)%>%
autoplot
roc_plot <-
stranded_preds %>%
roc_curve(truth = stranded_class, stranded_preds$LR_Class)%>%
autoplot
roc_plot <-
stranded_preds %>%
roc_curve(truth = stranded_class, LR_StrandedProb) %>%
autoplot
print(roc_plot)
roc_plot <-
stranded_preds %>%
roc_curve(truth = stranded_class, LR_NotStrandedProb) %>%
autoplot
print(roc_plot)
library(caret)
cm <- caret::confusionMatrix(stranded_preds$stranded_class,
stranded_preds$LR_Class)
print(cm)
library(ConfusionTableR)
cm <- ConfusionTableR::binary_visualiseR(
train_labels = stranded_preds$stranded_class,
truth_labels = stranded_preds$LR_Class,
class_label1 = "Not Stranded",
class_label2 = "Stranded",
quadrant_col1 = "#28ACB4",
quadrant_col2 = "#4397D2",
custom_title = "Stranded Patient Confusion Matrix",
text_col= "black",
cm_stat_size = 1.2,
round_dig = 2)
cm_binary_class <- ConfusionTableR::binary_class_cm(
train_labels = stranded_preds$stranded_class,
truth_labels = stranded_preds$LR_Class)
# Expose the record level confusion matrix
glimpse(cm_binary_class$record_level_cm)
save.image(file="Data/stranded_data.rdata")
load(file="Data/stranded_data.rdata")
set.seed(123)
#Set a random seed for replication of results
ten_fold <- vfold_cv(train_data, v=10)
set.seed(123)
lr_fit_rs <-
strand_wf %>%
fit_resamples(ten_fold)
# To collect the resmaples you need to call collect_metrics to average out the accuracy for that model
collected_mets <- tune::collect_metrics(lr_fit_rs)
print(collected_mets)
# Now I can compare the accuracy from the previous test set I had already generated a confusion matrix for
accuracy_resamples <- collected_mets$mean[1] * 100
accuracy_validation_set <- as.numeric(cm$overall[1] * 100)
print(cat(paste0("The true accuracy of the model is between the resample testing:",
round(accuracy_resamples,2), "\nThe validation sample: ",
round(accuracy_validation_set,2), ".")))
rf_mod <-
rand_forest(trees=500) %>%
set_engine("ranger") %>%
set_mode("classification")
print(rf_mod)
rf_fit <-
rf_mod %>%
fit(stranded_class ~ ., data = train_data)
print(rf_fit)
#Create workflow step
rf_wf <-
workflow() %>%
add_model(rf_mod) %>%
add_formula(stranded_class ~ .) #The predictor is contained in add_formula method
set.seed(123)
rf_fit_rs <-
rf_wf %>%
fit_resamples(ten_fold)
print(rf_fit_rs)
# Collect the metrics using another model with resampling
rf_resample_mean_preds <- tune::collect_metrics(rf_fit_rs)
print(rf_resample_mean_preds)
tune_tree <-
decision_tree(
cost_complexity = tune(), #tune() is a placeholder for an empty grid
tree_depth = tune() #we will fill these in the next section
) %>%
set_engine("rpart") %>%
set_mode("classification")
print(tune_tree)
grid_tree_tune <- grid_regular(dials::cost_complexity(),
dials::tree_depth(),
levels = 10)
print(head(grid_tree_tune,20))
all_cores <- parallel::detectCores(logical = FALSE)-1
print(all_cores)
#Registers all cores and subtracts one, so you have some time to work
cl <- makePSOCKcluster(all_cores)
print(cl)
#Makes an in memory cluster to utilise your cores
registerDoParallel(cl)
#Registers that we want to do parallel processing
set.seed(123)
tree_wf <- workflow() %>%
add_model(tune_tree) %>%
add_formula(stranded_class ~ .)
# Make the decision tree workflow - always postfix with wf for convention
# Add the registered model
# Add the formula of the outcome class you are predicting against all IVs
tree_pred_tuned <-
tree_wf %>%
tune::tune_grid(
resamples = ten_fold, #This is the 10 fold cross validation variable we created earlier
grid = grid_tree_tune #This is the tuning grid
)
tune_plot <- tree_pred_tuned %>%
collect_metrics() %>% #Collect metrics from tuning
mutate(tree_depth = factor(tree_depth)) %>%
ggplot(aes(cost_complexity, mean, color = tree_depth)) +
geom_line(size = 1, alpha = 0.7) +
geom_point(size = 1.5) +
facet_wrap(~ .metric, scales = "free", nrow = 2) +
scale_x_log10(labels = scales::label_number()) +
scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + theme_minimal()
print(tune_plot)
ggsave(filename="Figures/hyperparameter_tree.png", tune_plot)
# To get the best ROC - area under the curve value we will use the following:
tree_pred_tuned %>%
tune::show_best("roc_auc")
# Select the best tree
best_tree <- tree_pred_tuned %>%
tune::select_best("roc_auc")
print(best_tree)
final_wf <-
tree_wf %>%
finalize_workflow(best_tree) #Finalise workflow passes in our best tree
print(final_wf)
final_tree_pred <-
final_wf %>%
fit(data = train_data)
print(final_tree_pred)
plot <- final_tree_pred %>%
pull_workflow_fit() %>%
vip(aesthetics = list(color = "black", fill = "#26ACB5")) + theme_minimal()
print(plot)
ggsave("Figures/VarImp.png", plot)
plot <- final_tree_pred %>%
extract_fit_parsnip() %>%
vip(aesthetics = list(color = "black", fill = "#26ACB5")) + theme_minimal()
print(plot)
ggsave("Figures/VarImp.png", plot)
# Create the final prediction
final_fit <-
final_wf %>%
last_fit(split)
final_fit_fitted_metrics <- final_fit %>%
collect_metrics()
print(final_fit_fitted_metrics)
#Create the final predictions
final_fit_predictions <- final_fit %>%
collect_predictions()
print(final_fit_predictions)
roc_plot <- final_fit_predictions %>%
roc_curve(stranded_class, `.pred_Not Stranded`) %>%
autoplot()
print(roc_plot)
ggsave(filename = "Figures/tuned_tree.png", plot=roc_plot)
args(decision_tree)
args(logistic_reg)
args(rand_forest)
args(decision_tree)
args(logistic_reg)
args(rand_forest)
# Use stranded recipe
# Prepare the recipe
strand_rec_preped <- stranded_rec %>%
prep()
#Bake the recipe
strand_folds <-
recipes::bake(
strand_rec_preped,
new_data = training(split)
) %>%
rsample::vfold_cv(v = 10)
xgboost_model <-
parsnip::boost_tree(
mode = "classification",
trees=1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params, size = 100 #Indicates the size of the search space
)
xgboost_grid
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(stranded_class ~ .)
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples  = strand_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, roc_auc),
control = tune::control_grid(verbose=TRUE)
)
roc_plot <- final_fit_predictions %>%
roc_curve(stranded_class, `.pred_Stranded`) %>%
autoplot()
