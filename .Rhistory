set.seed(123)
#Set a random seed for replication of results
ten_fold <- vfold_cv(train_data, v=10)
set.seed(123)
lr_fit_rs <-
strand_wf %>%
fit_resamples(ten_fold)
rf_mod <-
rand_forest(trees=500) %>%
set_engine("ranger") %>%
set_mode("classification")
print(rf_mod)
rf_fit <-
rf_mod %>%
fit(stranded_class ~ ., data = train_data)
print(rf_fit)
#Create workflow step
rf_wf <-
workflow() %>%
add_model(rf_mod) %>%
add_formula(stranded_class ~ .) #The predictor is contained in add_formula method
set.seed(123)
rf_fit_rs <-
rf_wf %>%
fit_resamples(ten_fold)
print(rf_fit_rs)
# Collect the metrics using another model with resampling
rf_resample_mean_preds <- tune::collect_metrics(rf_fit_rs)
print(rf_resample_mean_preds)
tune_tree <-
decision_tree(
cost_complexity = tune(), #tune() is a placeholder for an empty grid
tree_depth = tune() #we will fill these in the next section
) %>%
set_engine("rpart") %>%
set_mode("classification")
print(tune_tree)
grid_tree_tune <- grid_regular(dials::cost_complexity(),
dials::tree_depth(),
levels = 10)
print(head(grid_tree_tune,20))
all_cores <- parallel::detectCores(logical = FALSE)-1
print(all_cores)
#Registers all cores and subtracts one, so you have some time to work
cl <- makePSOCKcluster(all_cores)
print(cl)
#Makes an in memory cluster to utilise your cores
registerDoParallel(cl)
#Registers that we want to do parallel processing
set.seed(123)
tree_wf <- workflow() %>%
add_model(tune_tree) %>%
add_formula(stranded_class ~ .)
# Make the decision tree workflow - always postfix with wf for convention
# Add the registered model
# Add the formula of the outcome class you are predicting against all IVs
tree_pred_tuned <-
tree_wf %>%
tune::tune_grid(
resamples = ten_fold, #This is the 10 fold cross validation variable we created earlier
grid = grid_tree_tune #This is the tuning grid
)
tune_plot <- tree_pred_tuned %>%
collect_metrics() %>% #Collect metrics from tuning
mutate(tree_depth = factor(tree_depth)) %>%
ggplot(aes(cost_complexity, mean, color = tree_depth)) +
geom_line(size = 1, alpha = 0.7) +
geom_point(size = 1.5) +
facet_wrap(~ .metric, scales = "free", nrow = 2) +
scale_x_log10(labels = scales::label_number()) +
scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + theme_minimal()
print(tune_plot)
ggsave(filename="Figures/hyperparameter_tree.png", tune_plot)
# To get the best ROC - area under the curve value we will use the following:
tree_pred_tuned %>%
tune::show_best("roc_auc")
# Select the best tree
best_tree <- tree_pred_tuned %>%
tune::select_best("roc_auc")
print(best_tree)
final_wf <-
tree_wf %>%
finalize_workflow(best_tree) #Finalise workflow passes in our best tree
print(final_wf)
final_tree_pred <-
final_wf %>%
fit(data = train_data)
print(final_tree_pred)
final_tree_pred <-
final_wf %>%
fit(data = train_data)
print(final_tree_pred)
plot <- final_tree_pred %>%
pull_workflow_fit() %>%
vip(aesthetics = list(color = "black", fill = "#26ACB5")) + theme_minimal()
print(plot)
ggsave("Figures/VarImp.png", plot)
# Create the final prediction
final_fit <-
final_wf %>%
last_fit(split)
final_fit_fitted_metrics <- final_fit %>%
collect_metrics()
print(final_fit_fitted_metrics)
#Create the final predictions
final_fit_predictions <- final_fit %>%
collect_predictions()
print(final_fit_predictions)
roc_plot <- final_fit_predictions %>%
roc_curve(stranded_class, `.pred_Not Stranded`) %>%
autoplot()
print(roc_plot)
ggsave(filename = "Figures/tuned_tree.png", plot=roc_plot)
args(decision_tree)
args(logistic_reg)
args(rand_forest)
registerDoParallel(cl)
#Register cluster for parallel processing
lr_model <- logistic_reg() %>%
set_mode("classification") %>%
set_engine("glm")
rf_model <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
xg_boost_model <- boost_tree() %>%
set_mode("classification") %>%
set_engine("xgboost")
# You could tune hyperparameters here, see previous step, for simplicity I am
# just instantiating the models
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xg_boost_model)
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
)
rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
)
xgboost_fit <- fit_resamples(
xgboost_wf,
resamples = ten_fold,
control = model_control
)
library(stacks)
meta_stacked_model <- stacks() %>%
add_candidates(lr_fit) %>%
add_candidates(rf_fit) %>%
add_candidates(xgboost_fit)
print(meta_stacked_model)
as_tibble(meta_stacked_model)
meta_stacked_model <- meta_stacked_model %>%
blend_predictions()
print(meta_stacked_model)
theme_set(theme_minimal())
autoplot(meta_stacked_model)
autoplot(meta_stacked_model, type="members")
autoplot(meta_stacked_model, type="weights")
meta_stacked_model <- meta_stacked_model %>%
fit_members()
print(meta_stacked_model)
registerDoParallel(cl)
#Register cluster for parallel processing
meta_stacked_model <- meta_stacked_model %>%
fit_members()
print(meta_stacked_model)
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(meta_stacked_model <- meta_stacked_model %>%
fit_members())
print(meta_stacked_model)
test_data <-
test_data %>%
bind_cols(predict(meta_stacked_model, .))
print(test_data)
test_data <-
test_data %>%
bind_cols(predict(meta_stacked_model, .),
predict(meta_stacked_model, ., type="prob"))
print(test_data)
# Partition into training and hold out test / validation sample
set.seed(123)
split <- rsample::initial_split(strand_pat, prop=3/4)
train_data <- rsample::training(split)
test_data <- rsample::testing(split)
test_data <-
test_data %>%
bind_cols(predict(meta_stacked_model, .),
predict(meta_stacked_model, ., type="prob")) #Expose the prediction probabilities
print(test_data)
plot <- ggplot(data=test_data, aes(x=stranded_class, y=.pred_class)) +
geom_point()
print(plot)
plot <- ggplot(data=test_data, aes(x=stranded_class, y=.pred_class)) +
geom_point()
print(plot) + coord_obs_pred()
cm <- caret::confusionMatrix(test_data$stranded_class,
test_data$.pred_class,
positive="Stranded")
print(cm)
lr_model <- logistic_reg() %>%
set_mode("classification") %>%
set_engine("glm")
rf_model <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
xg_boost_model <- boost_tree() %>%
set_mode("classification") %>%
set_engine("xgboost")
nn_model <- mlp(epochs=300, hidden_units = 5, dropout = 0.5) %>%
set_mode("classification") %>%
set_engine("keras", verbose=0)
# You could tune hyperparameters here, see previous step, for simplicity I am
# just instantiating the models
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
nn_wf <- workflow() %>%
add_model(nn_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xg_boost_model)
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
nn_wf <- workflow() %>%
add_model(nn_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xg_boost_wf)
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
nn_wf <- workflow() %>%
add_model(nn_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xgboost_wf)
print(nn_wf)
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
)
rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
)
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
))
system.time(rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
))
system.time(xgboost_fit <- fit_resamples(
xgboost_wf,
resamples = ten_fold,
control = model_control
))
system.time(nn_fit <- fit_resamples(
nn_fit,
resamples = ten_fold,
control = model_control
))
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
))
system.time(rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
))
system.time(xgboost_fit <- fit_resamples(
xgboost_wf,
resamples = ten_fold,
control = model_control
))
system.time(nn_fit <- fit_resamples(
nn_wf,
resamples = ten_fold,
control = model_control
))
library(stacks)
meta_stacked_model <- stacks() %>%
add_candidates(lr_fit) %>%
add_candidates(rf_fit) %>%
add_candidates(xgboost_fit) %>%
add_candidates(nn_fit)
print(meta_stacked_model)
as_tibble(meta_stacked_model)
meta_stacked_model <- meta_stacked_model %>%
blend_predictions()
print(meta_stacked_model)
theme_set(theme_minimal())
autoplot(meta_stacked_model)
autoplot(meta_stacked_model, type="members")
autoplot(meta_stacked_model, type="weights")
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(meta_stacked_model <- meta_stacked_model %>%
fit_members())
print(meta_stacked_model)
lr_model <- logistic_reg() %>%
set_mode("classification") %>%
set_engine("glm")
rf_model <- rand_forest() %>%
set_mode("classification") %>%
set_engine("ranger")
xg_boost_model <- boost_tree() %>%
set_mode("classification") %>%
set_engine("xgboost")
nn_model <- mlp(epochs=300, hidden_units = 5, dropout = 0.5) %>%
set_mode("classification") %>%
set_engine("keras", verbose=0)
neighbour_model <- nearest_neighbor() %>%
set_engine("kknn") %>%
set_mode("classification")
# You could tune hyperparameters here, see previous step, for simplicity I am
# just instantiating the models
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
nn_wf <- workflow() %>%
add_model(nn_model) %>%
add_recipe(stranded_rec)
neighbour_wf <- workflow() %>%
add_model(neighbour_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xgboost_wf)
print(nn_wf)
lr_wf <- workflow() %>%
add_model(lr_model) %>%
add_recipe(stranded_rec) #Use the stranded recipe we created once at the top
rf_wf <- workflow() %>%
add_model(rf_model) %>%
add_recipe(stranded_rec)
xgboost_wf <- workflow() %>%
add_model(xg_boost_model) %>%
add_recipe(stranded_rec)
nn_wf <- workflow() %>%
add_model(nn_model) %>%
add_recipe(stranded_rec)
neighbour_wf <- workflow() %>%
add_model(neighbour_model) %>%
add_recipe(stranded_rec)
print(lr_wf)
print(rf_wf)
print(xgboost_wf)
print(nn_wf)
print(neighbour_wf)
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
))
system.time(rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
))
system.time(xgboost_fit <- fit_resamples(
xgboost_wf,
resamples = ten_fold,
control = model_control
))
system.time(nn_fit <- fit_resamples(
nn_wf,
resamples = ten_fold,
control = model_control
))
system.time(neighbours_fit <- fit_resamples(
neighbour_wf,
resamples = ten_fold,
control = model_control
))
install.packages("kknn")
model_control <- control_grid(save_pred = TRUE, save_workflow = TRUE)
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(lr_fit <- fit_resamples(
lr_wf,
resamples = ten_fold,
control = model_control
))
system.time(rf_fit <- fit_resamples(
rf_wf,
resamples = ten_fold,
control = model_control
))
system.time(xgboost_fit <- fit_resamples(
xgboost_wf,
resamples = ten_fold,
control = model_control
))
system.time(nn_fit <- fit_resamples(
nn_wf,
resamples = ten_fold,
control = model_control
))
system.time(neighbours_fit <- fit_resamples(
neighbour_wf,
resamples = ten_fold,
control = model_control
))
library(stacks)
meta_stacked_model <- stacks() %>%
add_candidates(lr_fit) %>%
add_candidates(rf_fit) %>%
add_candidates(xgboost_fit) %>%
add_candidates(nn_fit) %>%
add_candidates(neighbours_fit)
print(meta_stacked_model)
as_tibble(meta_stacked_model)
meta_stacked_model <- meta_stacked_model %>%
blend_predictions()
print(meta_stacked_model)
theme_set(theme_minimal())
autoplot(meta_stacked_model)
autoplot(meta_stacked_model, type="members")
autoplot(meta_stacked_model, type="weights")
registerDoParallel(cl)
#Register cluster for parallel processing
system.time(meta_stacked_model <- meta_stacked_model %>%
fit_members())
print(meta_stacked_model)
test_data <-
test_data %>%
bind_cols(predict(meta_stacked_model, .),
predict(meta_stacked_model, ., type="prob")) #Expose the prediction probabilities
print(test_data)
# Partition into training and hold out test / validation sample
set.seed(123)
split <- rsample::initial_split(strand_pat, prop=3/4)
train_data <- rsample::training(split)
test_data <- rsample::testing(split)
test_data <-
test_data %>%
bind_cols(predict(meta_stacked_model, .),
predict(meta_stacked_model, ., type="prob")) #Expose the prediction probabilities
print(test_data)
cm <- caret::confusionMatrix(test_data$stranded_class,
test_data$.pred_class,
positive="Stranded")
print(cm)
View(test_data)
cm_plot <- ConfusionTableR::binary_visualiseR(cm, class_label1 = "Not Stranded",
class_label2 = "Stranded",
quadrant_col1 = "#53BFD3", quadrant_col2 = "#006838",
text_col = "white", custom_title = "Stranded patient Confusion Matrix")
