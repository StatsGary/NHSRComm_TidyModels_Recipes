---
title: "Improving a TidyModel for Classification"
author: "Gary Hutson - Head of Advanced Analytics"
date: "10/02/2021"
output:
  html_document:
    theme: lumen
    highlight: tango
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(readr)
library(broom)
library(broom.mixed)
library(skimr)
library(dplyr)
library(magrittr)
library(parallel)
library(doParallel)
library(vip)
library(pdp)
```

# Resuming where we left off in the first Markdown document

The first markdown document showed you how to build your first TidyModels model on an healthcare dataset. This could be a ML model you simply tweak for your own uses. I will now load the data back in and resume where we left off:

```{r load_data}
load(file="Data/stranded_data.rdata")

```

# Improve the model with resampling with the Rsample package

The first step will involve something called cross validation (see supporting workshop slides). The essence of cross validation is that you take sub samples of the training dataset. This is done to emulate how well the model will perform on unseen data samples when out in the wild (production):

![](Images/kfold.png "K fold cross validation")
As the image shows - the folds take a sampe of the training set and each randomly selected fold acts as the test sample. We then use a final hold out validation set to finally test the model. This will be shown in the following section. 

```{r kfold}
set.seed(123)
#Set a random seed for replication of results
ten_fold <- vfold_cv(train_data, v=10)

```

## Use previous workflow with cross validation

We will use the previous trained logistic regression model with resamples to improve the results of the cross validation:
```{r resamples_on_log_mod}
set.seed(123)
lr_fit_rs <- 
  strand_wf %>% 
  fit_resamples(ten_fold)

```

We will now collect the metrics using the tune package and the collect_metrics function:

```{r resamples_collmets}
# To collect the resmaples you need to call collect_metrics to average out the accuracy for that model
collected_mets <- tune::collect_metrics(lr_fit_rs)
print(collected_mets)
# Now I can compare the accuracy from the previous test set I had already generated a confusion matrix for
accuracy_resamples <- collected_mets$mean[1] * 100
accuracy_validation_set <- as.numeric(cm$overall[1] * 100)
print(cat(paste0("The true accuracy of the model is between the resample testing:", 
            round(accuracy_resamples,2), "\nThe validation sample: ",
            round(accuracy_validation_set,2), ".")))

```

This shows that the true accuracy value is somewhere between the reported results from the resampling method and those in our validation sample. 

# Improve the model with different model selection and resampling

The following example will move on from the logistic regression and aim to build a random forest, and later a decision tree. Other options in Parnsip would be to use a gradient boosted tree to amp up the results further. In addition, I aim at teaching a follow up webinar to this for ensembling - specifically model stacking (Stacks package) and bagging (Baguette package).

## Define and instantiate the model

The first step, as with the logistic regression example, if to define and instantiate the model:

```{r define_mod}
rf_mod <- 
  rand_forest(trees=500) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

print(rf_mod)
```

## Fit the model to the previous training data

Then we are going to fit the model to the previous training data:

```{r fit_model}
rf_fit <- 
  rf_mod %>% 
  fit(stranded_class ~ ., data = train_data)

print(rf_fit)
```

## Improve further by fitting to resamples

We will aim to increase the sample representation in this model by fitting it to a resamples object, in parsnip and rsample:
```{r fit_model_rsampl}
#Create workflow step
rf_wf <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_formula(stranded_class ~ .) #The predictor is contained in add_formula method

set.seed(123)
rf_fit_rs <- 
  rf_wf %>% 
  fit_resamples(ten_fold)

print(rf_fit_rs)
```
## Collect the resampled metrics

The next step is to collect the resample metrics:

```{r fit_model_rsamples_preds}
# Collect the metrics using another model with resampling
rf_resample_mean_preds <- tune::collect_metrics(rf_fit_rs)
print(rf_resample_mean_preds)
```

The model predictive power is maxing out at about 78%. I know this is due to the fact that the data is dummy data and most of the features that are contained in the model have a weak association to the outcome variable. 

What you would need to do after this is look for more representative features of what causes a patient to stay a long time in hospital. This is where the clinical context comes into play.

# Improve the model with hyperparameter tuning with the Dials package

We are going to now create a decision tree and we are going to tune the hyperparameters using the dials package. The dials package contains a list of hyperparameter tuning methods and is useful for creating quick hyperparameter grids and aiming to optimise them. 

## Building the decision tree

Like all the other steps, the first thing to do is build the decision tree. Note - the reason set_model("classification") is because the thing we are predicting is a factor. If this was a continuous variable, then you would need to switch this to regression. However, the model development for regression is identical to classification.

```{r build_decision_tree}
tune_tree <- 
  decision_tree(
    cost_complexity = tune(), #tune() is a placeholder for an empty grid 
    tree_depth = tune() #we will fill these in the next section
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

print(tune_tree)
```

## Create the hyperparameter grid search

The next step is to fill these blank values for cost complexity and tree depth - see the documentation for parsnip about these meaning, but decision trees have a cost value which minimises the splits and the depth of the tree is how far down you go. 

We will now create the object:

```{r build_tuning}
grid_tree_tune <- grid_regular(dials::cost_complexity(),
                               dials::tree_depth(), 
                               levels = 10)
print(head(grid_tree_tune,20))
```

## Setting up parallel processing

The tuning process, and modelling process, normally needs the ML engineer to access the full potential of your machine. The next steps show how to register the cores on your machine and max them out for training the model and doing grid searching:
```{r parallelproc}
all_cores <- parallel::detectCores(logical = FALSE)-1
print(all_cores)
#Registers all cores and subtracts one, so you have some time to work
cl <- makePSOCKcluster(all_cores)
print(cl)
#Makes an in memory cluster to utilise your cores
registerDoParallel(cl)
#Registers that we want to do parallel processing
```

## Creating the model workflow

Next, I will create the model workflow, as we have done a few times before:
```{r parallelproc2}
set.seed(123)
tree_wf <- workflow() %>% 
  add_model(tune_tree) %>% 
  add_formula(stranded_class ~ .)
# Make the decision tree workflow - always postfix with wf for convention
# Add the registered model
# Add the formula of the outcome class you are predicting against all IVs

tree_pred_tuned <- 
  tree_wf %>% 
  tune::tune_grid(
    resamples = ten_fold, #This is the 10 fold cross validation variable we created earlier
    grid = grid_tree_tune #This is the tuning grid
  )

```

## Visualise the tuning process

This ggplot helps to visualise how the manual tuning has gone on and will show where the best tree depth occurs in terms of the cost complexity (the number of terminal or leaf nodes):

```{r tune_visual}
tune_plot <- tree_pred_tuned %>%
  collect_metrics() %>% #Collect metrics from tuning
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1, alpha = 0.7) +
  geom_point(size = 1.5) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + theme_minimal()

print(tune_plot)
ggsave(filename="Figures/hyperparameter_tree.png", tune_plot)

```

This shows that you only need a depth of 4 to get the optimal accuracy. However, the tune package helps us out with this as well. 

## Selecting the best model from the tuning process with Tune

The tune package allows us to select the best candidate model, with the most optimal set of hyperparameters:

```{r tune_best}
# To get the best ROC - area under the curve value we will use the following:
tree_pred_tuned %>% 
  tune::show_best("roc_auc")

# Select the best tree
best_tree <- tree_pred_tuned %>% 
  tune::select_best("roc_auc")

print(best_tree)

```

The next step is to us the best tree to make our predictions.

## Using best tree to make predictions

```{r tune_best_select}
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree) #Finalise workflow passes in our best tree

print(final_wf)

```

Make a prediction against this finalised tree:

```{r tune_best_selected}
final_tree_pred <- 
  final_wf %>% 
  fit(data = train_data)

print(final_tree_pred)

```

## Use VIP package to visualise variable importance

We will look at global variable importance. As mentioned prior, to look at local patient level importance, use the LIME package.

```{r wf_fit}
plot <- final_tree_pred %>% 
  pull_workflow_fit() %>% 
  vip(aesthetics = list(color = "black", fill = "#26ACB5")) + theme_minimal()

print(plot)
ggsave("Figures/VarImp.png", plot)

```
This was derived when we looked at the logistic regression significance that these would be the important variables, due to their linear significance. 

## Create the final predictions

The last step is to create the final predictions from the tuned decision tree:

```{r final_fitted}
# Create the final prediction
final_fit <- 
  final_wf %>% 
  last_fit(split)

final_fit_fitted_metrics <- final_fit %>% 
  collect_metrics() 

print(final_fit_fitted_metrics)

#Create the final predictions
final_fit_predictions <- final_fit %>% 
  collect_predictions()
print(final_fit_predictions)


```

## Visualise the final fit on a ROC curve

You could do similar with viewing this object in the confusion matrix add in, but I will view this on a plot:

```{r final_fit}
roc_plot <- final_fit_predictions %>% 
  roc_curve(stranded_class, `.pred_Not Stranded`) %>% 
  autoplot()

print(roc_plot)
ggsave(filename = "Figures/tuned_tree.png", plot=roc_plot)

```

# Inspecting any Parsnip object

One last point to note - to inspect any of the tuning parameters and hyperparameters for the models you can use the args function to return these - examples below:

```{r parsnip_objects}
args(decision_tree)
args(logistic_reg)
args(rand_forest)

```

# Ensembling

If you are interested in a further session on ensembling - then I would be happy to go over the Stacks and Baguette packages for model stacking and bagging. These are relatively new additions to TidyModels and they are not as optimised as some of the caret packages, but I would be happy to show you how these are implemented. 
